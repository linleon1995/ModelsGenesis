{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processed LIDC data can be found at: https://drive.google.com/drive/folders/1TLpPvR_9hfNdUbD9dFIXNpJ7m50VmD19?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "from __future__ import print_function\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'  # or any {'0', '1', '2'}\n",
    "import keras\n",
    "print(\"keras = {}\".format(keras.__version__))\n",
    "import tensorflow as tf\n",
    "print(\"tensorflow-gpu = {}\".format(tf.__version__))\n",
    "try:\n",
    "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "except:\n",
    "    pass\n",
    "import random\n",
    "import shutil\n",
    "import argparse\n",
    "import sklearn\n",
    "from pathlib import Path\n",
    "from utils import *\n",
    "from unet3d import *\n",
    "from config import *\n",
    "from ncs_data import *\n",
    "\n",
    "class set_args():\n",
    "    gpu = 0\n",
    "    data = None\n",
    "    apps = 'ncs'\n",
    "    run = 3\n",
    "    cv = None\n",
    "    subsetting = None\n",
    "    suffix = 'genesis'\n",
    "    task = 'segmentation'\n",
    "    \n",
    "args = set_args()\n",
    "\n",
    "if args.gpu is not None:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu)\n",
    "    \n",
    "\n",
    "conf = ncs_config(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = '32x64x64-10-shift-8'\n",
    "input_roots = [\n",
    "            os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Malignant', 'crop', key, 'positive', 'Image'),\n",
    "            os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Malignant', 'crop', key, 'positive', 'Image'),\n",
    "        os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Benign', 'crop', key, 'positive', 'Image'),\n",
    "        os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Benign', 'crop', key, 'positive', 'Image'),\n",
    "            os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Malignant', 'crop', key, 'negative', 'Image'),\n",
    "            os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Malignant', 'crop', key, 'negative', 'Image'),\n",
    "        os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Benign', 'crop', key, 'negative', 'Image'),\n",
    "        os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Benign', 'crop', key, 'negative', 'Image'),\n",
    "            ]\n",
    "target_roots = [\n",
    "            os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Malignant', 'crop', key, 'positive', 'Mask'),\n",
    "            os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Malignant', 'crop', key, 'positive', 'Mask'),\n",
    "            os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Benign', 'crop', key, 'positive', 'Mask'),\n",
    "            os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Benign', 'crop', key, 'positive', 'Mask'),\n",
    "            os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Malignant', 'crop', key, 'negative', 'Mask'),\n",
    "            os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Malignant', 'crop', key, 'negative', 'Mask'),\n",
    "            os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Benign', 'crop', key, 'negative', 'Mask'),\n",
    "            os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Benign', 'crop', key, 'negative', 'Mask'),\n",
    "                ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_keys = [f'1m{idx:04d}' for idx in range(1, 37)] + [f'1B{idx:04d}' for idx in range(1, 21)]\n",
    "valid_file_keys = [f'1m{i:04d}' for i in range(37, 39)] + [f'1B{idx:04d}' for idx in range(21, 23)]\n",
    "test_file_keys = [f'1m{i:04d}' for i in range(37, 45)] + [f'1B{idx:04d}' for idx in range(21, 26)]\n",
    "\n",
    "\n",
    "train_input_samples = get_samples(input_roots, train_file_keys)   \n",
    "train_target_samples = get_samples(target_roots, train_file_keys) \n",
    "x_train, y_train = load_data(train_input_samples, train_target_samples, remove_zeros=conf.remove_zeros)\n",
    "x_train = x_train[:,np.newaxis]\n",
    "y_train = y_train[:,np.newaxis]\n",
    "\n",
    "valid_input_samples = get_samples(input_roots, valid_file_keys)   \n",
    "valid_target_samples = get_samples(target_roots, valid_file_keys) \n",
    "x_valid, y_valid = load_data(valid_input_samples, valid_target_samples, remove_zeros=conf.remove_zeros)\n",
    "x_valid = x_valid[:,np.newaxis]\n",
    "y_valid = y_valid[:,np.newaxis]\n",
    "\n",
    "test_input_samples = get_samples(input_roots, test_file_keys)   \n",
    "test_target_samples = get_samples(target_roots, test_file_keys) \n",
    "x_test, y_test = load_data(test_input_samples, test_target_samples, remove_zeros=False)\n",
    "x_test = x_test[:,np.newaxis]\n",
    "y_test = y_test[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "print('x_train: {} | {} ~ {}'.format(x_train.shape, np.min(x_train), np.max(x_train)))\n",
    "print('y_train: {} | {} ~ {}'.format(y_train.shape, np.min(y_train), np.max(y_train)))\n",
    "\n",
    "print('x_valid: {} | {} ~ {}'.format(x_valid.shape, np.min(x_valid), np.max(x_valid)))\n",
    "print('y_valid: {} | {} ~ {}'.format(y_valid.shape, np.min(y_valid), np.max(y_valid)))\n",
    "\n",
    "print('x_test: {} | {} ~ {}'.format(x_test.shape, np.min(x_test), np.max(x_test)))\n",
    "print('y_test: {} | {} ~ {}'.format(y_test.shape, np.min(y_test), np.max(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "train_zeros, valid_zeros, test_zeros = 0, 0, 0\n",
    "for y in y_train:\n",
    "    if np.sum(y) <= 0:\n",
    "        train_zeros += 1\n",
    "\n",
    "for y in y_valid:\n",
    "    if np.sum(y) <= 0:\n",
    "        valid_zeros += 1\n",
    "\n",
    "for y in y_test:\n",
    "    if np.sum(y) <= 0:\n",
    "        test_zeros += 1\n",
    "\n",
    "print('Zeros sample')\n",
    "print(f'Train {train_zeros} Valid {valid_zeros} Test {test_zeros}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Models Genesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "args.suffix = 'genesis'\n",
    "conf = ncs_config(args)\n",
    "conf.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "model = unet_model_3d((1,conf.input_rows,conf.input_cols,conf.input_deps), batch_normalization=True)\n",
    "if conf.weights is not None:\n",
    "    print(\"[INFO] Load pre-trained weights from {}\".format(conf.weights))\n",
    "    model.load_weights(conf.weights)\n",
    "model, callbacks = model_setup(model, conf, task=args.task)\n",
    "\n",
    "while conf.batch_size > 1:\n",
    "    # To find a largest batch size that can be fit into GPU\n",
    "    try:\n",
    "        model.fit(x_train, y_train,\n",
    "                  validation_data=(x_valid, y_valid),\n",
    "                  batch_size=conf.batch_size,\n",
    "                  epochs=conf.nb_epoch, \n",
    "                  verbose=conf.verbose, \n",
    "                  shuffle=True,\n",
    "                  callbacks=callbacks)\n",
    "        break\n",
    "    except tf.errors.ResourceExhaustedError as e:\n",
    "        conf.batch_size = int(conf.batch_size - 2)\n",
    "        print(\"\\n> Batch size = {}\".format(conf.batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "x_data, y_data = x_test, y_test\n",
    "model = unet_model_3d((1,conf.input_rows,conf.input_cols,conf.input_deps), batch_normalization=True)\n",
    "print(\"[INFO] Load trained model from {}\".format( os.path.join(conf.model_path, conf.exp_name+\".h5\") ))\n",
    "model.load_weights( os.path.join(conf.model_path, conf.exp_name+\".h5\") )\n",
    "\n",
    "p_test = segmentation_model_evaluation(model=model, config=conf, x=x_data, y=y_data, note=conf.exp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print('CWD', os.getcwd())\n",
    "p_test = np.squeeze(p_test)\n",
    "for i in range(0, x_test.shape[0], 1):\n",
    "    plot_image_truth_prediction(x_test[i], y_test[i], p_test[i], rows=5, cols=5, name=f'figures/tmh/img{i:03d}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.suffix = 'random'\n",
    "conf = ncs_config(args)\n",
    "conf.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet_model_3d((1,conf.input_rows,conf.input_cols,conf.input_deps), batch_normalization=True)\n",
    "if conf.weights is not None:\n",
    "    print(\"[INFO] Load pre-trained weights from {}\".format(conf.weights))\n",
    "    model.load_weights(conf.weights)\n",
    "model, callbacks = model_setup(model, conf, task=args.task)\n",
    "\n",
    "while conf.batch_size > 1:\n",
    "    # To find a largest batch size that can be fit into GPU\n",
    "    try:\n",
    "        model.fit(x_train, y_train,\n",
    "                  validation_data=(x_valid, y_valid),\n",
    "                  batch_size=conf.batch_size,\n",
    "                  epochs=conf.nb_epoch, \n",
    "                  verbose=conf.verbose, \n",
    "                  shuffle=True,\n",
    "                  callbacks=callbacks)\n",
    "        break\n",
    "    except tf.errors.ResourceExhaustedError as e:\n",
    "        conf.batch_size = int(conf.batch_size - 2)\n",
    "        print(\"\\n> Batch size = {}\".format(conf.batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet_model_3d((1,conf.input_rows,conf.input_cols,conf.input_deps), batch_normalization=True)\n",
    "print(\"[INFO] Load trained model from {}\".format( os.path.join(conf.model_path, conf.exp_name+\".h5\") ))\n",
    "model.load_weights( os.path.join(conf.model_path, conf.exp_name+\".h5\") )\n",
    "\n",
    "p_test = segmentation_model_evaluation(model=model, config=conf, x=x_test, y=y_test, note=conf.exp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = np.squeeze(p_test)\n",
    "for i in range(0, x_test.shape[0], 1):\n",
    "    plot_image_truth_prediction(x_test[i], y_test[i], p_test[i], rows=5, cols=5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
