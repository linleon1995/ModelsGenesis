{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processed LIDC data can be found at: https://drive.google.com/drive/folders/1TLpPvR_9hfNdUbD9dFIXNpJ7m50VmD19?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras = 2.2.4\n",
      "tensorflow-gpu = 1.15.0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "from __future__ import print_function\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'  # or any {'0', '1', '2'}\n",
    "import keras\n",
    "print(\"keras = {}\".format(keras.__version__))\n",
    "import tensorflow as tf\n",
    "print(\"tensorflow-gpu = {}\".format(tf.__version__))\n",
    "try:\n",
    "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "except:\n",
    "    pass\n",
    "import random\n",
    "import shutil\n",
    "import argparse\n",
    "import sklearn\n",
    "from pathlib import Path\n",
    "from utils import *\n",
    "from unet3d import *\n",
    "from config import *\n",
    "from ncs_data import *\n",
    "\n",
    "class set_args():\n",
    "    gpu = 0\n",
    "    data = None\n",
    "    apps = 'ncs'\n",
    "    run = 4\n",
    "    cv = None\n",
    "    subsetting = None\n",
    "    suffix = 'genesis'\n",
    "    task = 'segmentation'\n",
    "    \n",
    "args = set_args()\n",
    "\n",
    "if args.gpu is not None:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu)\n",
    "    \n",
    "\n",
    "conf = ncs_config(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = '32x64x64-10-shift-8'\n",
    "key = '32x64x64-10'\n",
    "input_roots = [\n",
    "            os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Malignant', 'crop', key, 'positive', 'Image'),\n",
    "            # os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Malignant', 'crop', key, 'positive', 'Image'),\n",
    "        os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Benign', 'crop', key, 'positive', 'Image'),\n",
    "        # os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Benign', 'crop', key, 'positive', 'Image'),\n",
    "            os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Malignant', 'crop', key, 'negative', 'Image'),\n",
    "        #     os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Malignant', 'crop', key, 'negative', 'Image'),\n",
    "        os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Benign', 'crop', key, 'negative', 'Image'),\n",
    "        # os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Benign', 'crop', key, 'negative', 'Image'),\n",
    "            ]\n",
    "target_roots = [\n",
    "            os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Malignant', 'crop', key, 'positive', 'Mask'),\n",
    "            # os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Malignant', 'crop', key, 'positive', 'Mask'),\n",
    "            os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Benign', 'crop', key, 'positive', 'Mask'),\n",
    "            # os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Benign', 'crop', key, 'positive', 'Mask'),\n",
    "            os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Malignant', 'crop', key, 'negative', 'Mask'),\n",
    "            # os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Malignant', 'crop', key, 'negative', 'Mask'),\n",
    "            os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Benign', 'crop', key, 'negative', 'Mask'),\n",
    "            # os.path.join(rf'C:\\Users\\test\\Desktop\\Leon\\Datasets\\ASUS_Nodules-preprocess\\ASUS-Benign', 'crop', key, 'negative', 'Mask'),\n",
    "                ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_keys = [f'1m{idx:04d}' for idx in range(1, 37)] + [f'1B{idx:04d}' for idx in range(1, 21)]\n",
    "valid_file_keys = [f'1m{i:04d}' for i in range(37, 39)] + [f'1B{idx:04d}' for idx in range(21, 23)]\n",
    "test_file_keys = [f'1m{i:04d}' for i in range(37, 45)] + [f'1B{idx:04d}' for idx in range(21, 26)]\n",
    "\n",
    "\n",
    "train_input_samples = get_samples(input_roots, train_file_keys)   \n",
    "train_target_samples = get_samples(target_roots, train_file_keys) \n",
    "x_train, y_train = load_data(train_input_samples, train_target_samples, remove_zeros=True)\n",
    "x_train = x_train[:,np.newaxis]\n",
    "y_train = y_train[:,np.newaxis]\n",
    "\n",
    "valid_input_samples = get_samples(input_roots, valid_file_keys)   \n",
    "valid_target_samples = get_samples(target_roots, valid_file_keys) \n",
    "x_valid, y_valid = load_data(valid_input_samples, valid_target_samples, remove_zeros=conf.remove_zeros)\n",
    "x_valid = x_valid[:,np.newaxis]\n",
    "y_valid = y_valid[:,np.newaxis]\n",
    "\n",
    "test_input_samples = get_samples(input_roots, test_file_keys)   \n",
    "test_target_samples = get_samples(target_roots, test_file_keys) \n",
    "# for x in test_input_samples:\n",
    "#     print(os.path.split(x)[1])\n",
    "x_test, y_test = load_data(test_input_samples, test_target_samples, remove_zeros=True)\n",
    "x_test = x_test[:,np.newaxis]\n",
    "y_test = y_test[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (830, 1, 64, 64, 32) | 0.0 ~ 1.0\n",
      "y_train: (830, 1, 64, 64, 32) | 0 ~ 1\n",
      "x_valid: (44, 1, 64, 64, 32) | 0.0 ~ 1.0\n",
      "y_valid: (44, 1, 64, 64, 32) | 0 ~ 1\n",
      "x_test: (19, 1, 64, 64, 32) | 0.0 ~ 1.0\n",
      "y_test: (19, 1, 64, 64, 32) | 0 ~ 1\n"
     ]
    }
   ],
   "source": [
    "print('x_train: {} | {} ~ {}'.format(x_train.shape, np.min(x_train), np.max(x_train)))\n",
    "print('y_train: {} | {} ~ {}'.format(y_train.shape, np.min(y_train), np.max(y_train)))\n",
    "\n",
    "print('x_valid: {} | {} ~ {}'.format(x_valid.shape, np.min(x_valid), np.max(x_valid)))\n",
    "print('y_valid: {} | {} ~ {}'.format(y_valid.shape, np.min(y_valid), np.max(y_valid)))\n",
    "\n",
    "print('x_test: {} | {} ~ {}'.format(x_test.shape, np.min(x_test), np.max(x_test)))\n",
    "print('y_test: {} | {} ~ {}'.format(y_test.shape, np.min(y_test), np.max(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeros sample\n",
      "Train 762 Valid 37 Test 163\n"
     ]
    }
   ],
   "source": [
    "train_zeros, valid_zeros, test_zeros = 0, 0, 0\n",
    "for y in y_train:\n",
    "    if np.sum(y) <= 0:\n",
    "        train_zeros += 1\n",
    "\n",
    "for y in y_valid:\n",
    "    if np.sum(y) <= 0:\n",
    "        valid_zeros += 1\n",
    "\n",
    "for y in y_test:\n",
    "    if np.sum(y) <= 0:\n",
    "        test_zeros += 1\n",
    "\n",
    "print('Zeros sample')\n",
    "print(f'Train {train_zeros} Valid {valid_zeros} Test {test_zeros}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Models Genesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "arch                           Vnet\n",
      "batch_size                     16\n",
      "data                           data/ncs\n",
      "exp_name                       Vnet-genesis\n",
      "input_cols                     64\n",
      "input_deps                     32\n",
      "input_rows                     64\n",
      "logs_path                      models/ncs\\run_5\\logs\n",
      "lr                             0.001\n",
      "max_queue_size                 1\n",
      "model_path                     models/ncs\\run_5\n",
      "nb_epoch                       10000\n",
      "optimizer                      adam\n",
      "patience                       50\n",
      "remove_zeros                   False\n",
      "verbose                        1\n",
      "weights                        pretrained_weights/Genesis_Chest_CT.h5\n",
      "workers                        1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args.suffix = 'genesis'\n",
    "conf = ncs_config(args)\n",
    "conf.display()\n",
    "conf.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Load pre-trained weights from pretrained_weights/Genesis_Chest_CT.h5\n",
      "Train on 830 samples, validate on 44 samples\n",
      "Epoch 1/10000\n",
      "\n",
      "> Batch size = 14\n",
      "Train on 830 samples, validate on 44 samples\n",
      "Epoch 1/10000\n",
      "\n",
      "> Batch size = 12\n",
      "Train on 830 samples, validate on 44 samples\n",
      "Epoch 1/10000\n",
      "\n",
      "> Batch size = 10\n",
      "Train on 830 samples, validate on 44 samples\n",
      "Epoch 1/10000\n",
      "\n",
      "> Batch size = 8\n",
      "Train on 830 samples, validate on 44 samples\n",
      "Epoch 1/10000\n",
      "  8/830 [..............................] - ETA: 19:22 - loss: 1.0000 - mean_iou: 0.4958 - dice_coef: 7.5583e-06\n",
      "> Batch size = 6\n",
      "Train on 830 samples, validate on 44 samples\n",
      "Epoch 1/10000\n",
      "830/830 [==============================] - 166s 200ms/step - loss: 0.9526 - mean_iou: 0.5039 - dice_coef: 0.0474 - val_loss: 0.9851 - val_mean_iou: 0.5057 - val_dice_coef: 0.0149\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.98512, saving model to models/ncs\\run_5\\Vnet-genesis.h5\n",
      "Epoch 2/10000\n",
      "830/830 [==============================] - 154s 185ms/step - loss: 0.7676 - mean_iou: 0.5047 - dice_coef: 0.2324 - val_loss: 0.4930 - val_mean_iou: 0.5076 - val_dice_coef: 0.5070\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.98512 to 0.49298, saving model to models/ncs\\run_5\\Vnet-genesis.h5\n",
      "Epoch 3/10000\n",
      "830/830 [==============================] - 155s 186ms/step - loss: 0.4036 - mean_iou: 0.5080 - dice_coef: 0.5964 - val_loss: 0.2730 - val_mean_iou: 0.5081 - val_dice_coef: 0.7270\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.49298 to 0.27302, saving model to models/ncs\\run_5\\Vnet-genesis.h5\n",
      "Epoch 4/10000\n",
      "830/830 [==============================] - 154s 186ms/step - loss: 0.4167 - mean_iou: 0.5080 - dice_coef: 0.5833 - val_loss: 0.2725 - val_mean_iou: 0.5079 - val_dice_coef: 0.7275\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.27302 to 0.27249, saving model to models/ncs\\run_5\\Vnet-genesis.h5\n",
      "Epoch 5/10000\n",
      "830/830 [==============================] - 154s 185ms/step - loss: 0.3761 - mean_iou: 0.5078 - dice_coef: 0.6239 - val_loss: 0.2725 - val_mean_iou: 0.5077 - val_dice_coef: 0.7275\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.27249 to 0.27245, saving model to models/ncs\\run_5\\Vnet-genesis.h5\n",
      "Epoch 6/10000\n",
      "830/830 [==============================] - 154s 185ms/step - loss: 0.3520 - mean_iou: 0.5075 - dice_coef: 0.6480 - val_loss: 0.2724 - val_mean_iou: 0.5074 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.27245 to 0.27244, saving model to models/ncs\\run_5\\Vnet-genesis.h5\n",
      "Epoch 7/10000\n",
      "830/830 [==============================] - 154s 186ms/step - loss: 0.4383 - mean_iou: 0.5073 - dice_coef: 0.5617 - val_loss: 0.2724 - val_mean_iou: 0.5071 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.27244 to 0.27244, saving model to models/ncs\\run_5\\Vnet-genesis.h5\n",
      "Epoch 8/10000\n",
      "830/830 [==============================] - 156s 187ms/step - loss: 0.3882 - mean_iou: 0.5070 - dice_coef: 0.6118 - val_loss: 0.2724 - val_mean_iou: 0.5068 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.27244 to 0.27244, saving model to models/ncs\\run_5\\Vnet-genesis.h5\n",
      "Epoch 9/10000\n",
      "830/830 [==============================] - 154s 185ms/step - loss: 0.4049 - mean_iou: 0.5067 - dice_coef: 0.5951 - val_loss: 0.2724 - val_mean_iou: 0.5065 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.27244\n",
      "Epoch 10/10000\n",
      "830/830 [==============================] - 153s 184ms/step - loss: 0.3736 - mean_iou: 0.5063 - dice_coef: 0.6264 - val_loss: 0.2724 - val_mean_iou: 0.5062 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.27244\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 11/10000\n",
      "830/830 [==============================] - 153s 184ms/step - loss: 0.4029 - mean_iou: 0.5061 - dice_coef: 0.5971 - val_loss: 0.2724 - val_mean_iou: 0.5060 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.27244 to 0.27244, saving model to models/ncs\\run_5\\Vnet-genesis.h5\n",
      "Epoch 12/10000\n",
      "830/830 [==============================] - 153s 185ms/step - loss: 0.3953 - mean_iou: 0.5058 - dice_coef: 0.6047 - val_loss: 0.2724 - val_mean_iou: 0.5057 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.27244 to 0.27244, saving model to models/ncs\\run_5\\Vnet-genesis.h5\n",
      "Epoch 13/10000\n",
      "830/830 [==============================] - 153s 184ms/step - loss: 0.4022 - mean_iou: 0.5056 - dice_coef: 0.5978 - val_loss: 0.2724 - val_mean_iou: 0.5055 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.27244\n",
      "Epoch 14/10000\n",
      "830/830 [==============================] - 153s 184ms/step - loss: 0.3814 - mean_iou: 0.5054 - dice_coef: 0.6186 - val_loss: 0.2724 - val_mean_iou: 0.5053 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.27244 to 0.27244, saving model to models/ncs\\run_5\\Vnet-genesis.h5\n",
      "Epoch 15/10000\n",
      "830/830 [==============================] - 152s 184ms/step - loss: 0.3811 - mean_iou: 0.5052 - dice_coef: 0.6189 - val_loss: 0.2724 - val_mean_iou: 0.5051 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.27244 to 0.27244, saving model to models/ncs\\run_5\\Vnet-genesis.h5\n",
      "Epoch 16/10000\n",
      "830/830 [==============================] - 152s 183ms/step - loss: 0.3885 - mean_iou: 0.5050 - dice_coef: 0.6115 - val_loss: 0.2724 - val_mean_iou: 0.5049 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.27244\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 17/10000\n",
      "830/830 [==============================] - 152s 183ms/step - loss: 0.4172 - mean_iou: 0.5048 - dice_coef: 0.5828 - val_loss: 0.2724 - val_mean_iou: 0.5047 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.27244\n",
      "Epoch 18/10000\n",
      "830/830 [==============================] - 153s 184ms/step - loss: 0.3909 - mean_iou: 0.5047 - dice_coef: 0.6091 - val_loss: 0.2724 - val_mean_iou: 0.5046 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.27244\n",
      "Epoch 19/10000\n",
      "830/830 [==============================] - 152s 183ms/step - loss: 0.4166 - mean_iou: 0.5045 - dice_coef: 0.5834 - val_loss: 0.2724 - val_mean_iou: 0.5044 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.27244\n",
      "Epoch 20/10000\n",
      "830/830 [==============================] - 152s 183ms/step - loss: 0.3833 - mean_iou: 0.5044 - dice_coef: 0.6167 - val_loss: 0.2724 - val_mean_iou: 0.5043 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.27244\n",
      "Epoch 21/10000\n",
      "830/830 [==============================] - 152s 183ms/step - loss: 0.4095 - mean_iou: 0.5042 - dice_coef: 0.5905 - val_loss: 0.2724 - val_mean_iou: 0.5041 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.27244\n",
      "Epoch 22/10000\n",
      "830/830 [==============================] - 153s 184ms/step - loss: 0.4027 - mean_iou: 0.5041 - dice_coef: 0.5973 - val_loss: 0.2724 - val_mean_iou: 0.5040 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.27244\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 23/10000\n",
      "830/830 [==============================] - 151s 182ms/step - loss: 0.3812 - mean_iou: 0.5040 - dice_coef: 0.6188 - val_loss: 0.2724 - val_mean_iou: 0.5039 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.27244\n",
      "Epoch 24/10000\n",
      "830/830 [==============================] - 151s 182ms/step - loss: 0.3952 - mean_iou: 0.5038 - dice_coef: 0.6048 - val_loss: 0.2724 - val_mean_iou: 0.5038 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.27244 to 0.27244, saving model to models/ncs\\run_5\\Vnet-genesis.h5\n",
      "Epoch 25/10000\n",
      "830/830 [==============================] - 151s 182ms/step - loss: 0.4311 - mean_iou: 0.5037 - dice_coef: 0.5689 - val_loss: 0.2724 - val_mean_iou: 0.5037 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.27244\n",
      "Epoch 26/10000\n",
      "830/830 [==============================] - 152s 184ms/step - loss: 0.4021 - mean_iou: 0.5036 - dice_coef: 0.5979 - val_loss: 0.2724 - val_mean_iou: 0.5036 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.27244\n",
      "Epoch 27/10000\n",
      "830/830 [==============================] - 152s 183ms/step - loss: 0.3879 - mean_iou: 0.5035 - dice_coef: 0.6121 - val_loss: 0.2724 - val_mean_iou: 0.5035 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.27244\n",
      "Epoch 28/10000\n",
      "830/830 [==============================] - 151s 183ms/step - loss: 0.4025 - mean_iou: 0.5034 - dice_coef: 0.5975 - val_loss: 0.2724 - val_mean_iou: 0.5034 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.27244\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 29/10000\n",
      "830/830 [==============================] - 152s 183ms/step - loss: 0.4101 - mean_iou: 0.5033 - dice_coef: 0.5899 - val_loss: 0.2724 - val_mean_iou: 0.5033 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.27244 to 0.27244, saving model to models/ncs\\run_5\\Vnet-genesis.h5\n",
      "Epoch 30/10000\n",
      "830/830 [==============================] - 152s 183ms/step - loss: 0.3959 - mean_iou: 0.5032 - dice_coef: 0.6041 - val_loss: 0.2724 - val_mean_iou: 0.5032 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.27244\n",
      "Epoch 31/10000\n",
      "830/830 [==============================] - 152s 183ms/step - loss: 0.4238 - mean_iou: 0.5031 - dice_coef: 0.5762 - val_loss: 0.2724 - val_mean_iou: 0.5031 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.27244\n",
      "Epoch 32/10000\n",
      "830/830 [==============================] - 151s 183ms/step - loss: 0.3877 - mean_iou: 0.5031 - dice_coef: 0.6123 - val_loss: 0.2724 - val_mean_iou: 0.5030 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.27244\n",
      "Epoch 33/10000\n",
      "830/830 [==============================] - 152s 183ms/step - loss: 0.3885 - mean_iou: 0.5030 - dice_coef: 0.6115 - val_loss: 0.2724 - val_mean_iou: 0.5029 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.27244\n",
      "Epoch 34/10000\n",
      "830/830 [==============================] - 152s 183ms/step - loss: 0.3950 - mean_iou: 0.5029 - dice_coef: 0.6050 - val_loss: 0.2724 - val_mean_iou: 0.5029 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.27244\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 35/10000\n",
      "830/830 [==============================] - 152s 183ms/step - loss: 0.4242 - mean_iou: 0.5028 - dice_coef: 0.5758 - val_loss: 0.2724 - val_mean_iou: 0.5028 - val_dice_coef: 0.7276\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.27244\n",
      "Epoch 36/10000\n",
      "750/830 [==========================>...] - ETA: 14s - loss: 0.3820 - mean_iou: 0.5028 - dice_coef: 0.6180"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d3fe4d91bbab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m                   \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                   \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                   callbacks=callbacks)\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResourceExhaustedError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplab\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplab\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplab\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = unet_model_3d((1,conf.input_rows,conf.input_cols,conf.input_deps), batch_normalization=True)\n",
    "if conf.weights is not None:\n",
    "    print(\"[INFO] Load pre-trained weights from {}\".format(conf.weights))\n",
    "    model.load_weights(conf.weights)\n",
    "model, callbacks = model_setup(model, conf, task=args.task)\n",
    "\n",
    "while conf.batch_size > 1:\n",
    "    # To find a largest batch size that can be fit into GPU\n",
    "    try:\n",
    "        model.fit(x_train, y_train,\n",
    "                  validation_data=(x_valid, y_valid),\n",
    "                  batch_size=conf.batch_size,\n",
    "                  epochs=conf.nb_epoch, \n",
    "                  verbose=conf.verbose, \n",
    "                  shuffle=True,\n",
    "                  callbacks=callbacks)\n",
    "        break\n",
    "    except tf.errors.ResourceExhaustedError as e:\n",
    "        conf.batch_size = int(conf.batch_size - 2)\n",
    "        print(\"\\n> Batch size = {}\".format(conf.batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Load trained model from models/ncs\\run_4\\Vnet-genesis.h5\n",
      "19/19 [==============================] - 6s 320ms/step\n",
      "Dice 51.11 %\n",
      "19/19 [==============================] - 2s 126ms/step\n",
      "[INFO] Vnet-genesis\n",
      "x:  (19, 1, 64, 64, 32) | 0.0 ~ 1.0\n",
      "y:  (19, 1, 64, 64, 32) | 0.0 ~ 1.0\n",
      "p:  (19, 1, 64, 64, 32) | 0.0 ~ 0.9\n",
      "[EVAL] Dice = 65.10%\n",
      "[EVAL] IoU  = 11.46%\n"
     ]
    }
   ],
   "source": [
    "x_data, y_data = x_train, y_train\n",
    "model = unet_model_3d((1,conf.input_rows,conf.input_cols,conf.input_deps), batch_normalization=True)\n",
    "print(\"[INFO] Load trained model from {}\".format( os.path.join(conf.model_path, conf.exp_name+\".h5\") ))\n",
    "model.load_weights( os.path.join(conf.model_path, conf.exp_name+\".h5\") )\n",
    "\n",
    "p_test = segmentation_model_evaluation(model=model, config=conf, x=x_data, y=y_data, note=conf.exp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print('CWD', os.getcwd())\n",
    "p_test = np.squeeze(p_test)\n",
    "for i in range(0, x_test.shape[0], 1):\n",
    "    plot_image_truth_prediction(x_test[i], y_test[i], p_test[i], rows=5, cols=5, name=f'figures/tmh/img{i:03d}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.suffix = 'random'\n",
    "conf = ncs_config(args)\n",
    "conf.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet_model_3d((1,conf.input_rows,conf.input_cols,conf.input_deps), batch_normalization=True)\n",
    "if conf.weights is not None:\n",
    "    print(\"[INFO] Load pre-trained weights from {}\".format(conf.weights))\n",
    "    model.load_weights(conf.weights)\n",
    "model, callbacks = model_setup(model, conf, task=args.task)\n",
    "\n",
    "while conf.batch_size > 1:\n",
    "    # To find a largest batch size that can be fit into GPU\n",
    "    try:\n",
    "        model.fit(x_train, y_train,\n",
    "                  validation_data=(x_valid, y_valid),\n",
    "                  batch_size=conf.batch_size,\n",
    "                  epochs=conf.nb_epoch, \n",
    "                  verbose=conf.verbose, \n",
    "                  shuffle=True,\n",
    "                  callbacks=callbacks)\n",
    "        break\n",
    "    except tf.errors.ResourceExhaustedError as e:\n",
    "        conf.batch_size = int(conf.batch_size - 2)\n",
    "        print(\"\\n> Batch size = {}\".format(conf.batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet_model_3d((1,conf.input_rows,conf.input_cols,conf.input_deps), batch_normalization=True)\n",
    "print(\"[INFO] Load trained model from {}\".format( os.path.join(conf.model_path, conf.exp_name+\".h5\") ))\n",
    "model.load_weights( os.path.join(conf.model_path, conf.exp_name+\".h5\") )\n",
    "\n",
    "p_test = segmentation_model_evaluation(model=model, config=conf, x=x_test, y=y_test, note=conf.exp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = np.squeeze(p_test)\n",
    "for i in range(0, x_test.shape[0], 1):\n",
    "    plot_image_truth_prediction(x_test[i], y_test[i], p_test[i], rows=5, cols=5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ade369081833004bd850ff4d77322467c6998d0218b3e4ddb64ebbb6e44ea06e"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('deeplab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
